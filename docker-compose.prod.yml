version: '3.8'

services:
  ml-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: food-calorie-ml-server-prod
    env_file:
      - .env
    ports:
      - "8001:8001"
    environment:
      # 프로덕션 환경 변수
      - DEBUG_MODE=false
      - ENABLE_MULTIMODAL=true
      - LLM_PROVIDER=gemini
      - LLM_MODEL_NAME=gemini-2.5-flash
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
    volumes:
      # 프로덕션에서는 로그만 호스트와 공유
      - ./logs:/app/logs
      # 결과 파일은 컨테이너 내부에 저장 (필요시 볼륨 마운트)
      - ml-results:/app/results
      - ml-data:/app/data
      - ml-weights:/app/weights
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - food-calorie-network
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'

  # Nginx 리버스 프록시 (선택사항)
  nginx:
    image: nginx:alpine
    container_name: ml-server-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - ml-server
    restart: always
    networks:
      - food-calorie-network
    profiles:
      - nginx

volumes:
  ml-results:
  ml-data:
  ml-weights:

networks:
  food-calorie-network:
    driver: bridge 